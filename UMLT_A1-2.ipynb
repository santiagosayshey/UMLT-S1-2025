{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe98470-f20c-4017-8970-0fc425f0ca4b",
   "metadata": {},
   "source": [
    "# Using Machine Learning Tools Assignment 1\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this assignment, you will apply some popular machine learning techniques to the problem of predicting bike rental demand. A data set has been provided containing records of bike rentals in Seoul, collected during 2017-18.\n",
    "\n",
    "## General instructions\n",
    "\n",
    "This assignment is divided into several tasks. Use the spaces provided in this notebook to answer the questions posed in each task. Note that some questions require writing a small amount of code and some require graphical results. It is your responsibility to make sure your responses are clearly labelled and your code has been fully executed (with the correct results displayed) before submission!\n",
    "\n",
    "**Do not** manually edit the data set file we have provided! For marking purposes, it's important that your code is written to run correctly on the original data file.\n",
    "\n",
    "When creating graphical output, label is clearly, with appropriate titles, xlabels and ylabels, as appropriate.\n",
    "\n",
    "Chapter 2 of the textbook is based on a similar workflow to this assignment, so you may look there for some further background and ideas. You can also use any other general resources on the internet that are relevant although do not use ones which directly relate to these questions with this dataset (which would normally only be found in someone else's assignment answers). If you take a large portion of code or text from the internet then you should reference where this was taken from, but we do not expect any references for small pieces of code. Taking, and adapting, small portions of code is expected and is common practice when solving real problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a08e03-e275-4ab1-80bd-8128e15a58c7",
   "metadata": {},
   "source": [
    "## The following code imports some of the essential libraries that you will need. You should not need to modify it, but you are expected to import other libraries as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c262c3-9ded-4f98-aafa-62b3d8c63d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb67336-c1a7-4592-afca-381db3d39261",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP01:** \n",
    "Load the data set from the csv file (SeoulBikeData.csv) into a DataFrame, and summarise it with the pandas functions `describe()` and `info()`.\n",
    "\n",
    "Download the data set from MyUni using the link provided on the assignment page. A paper that describes one related version of this dataset is: Sathishkumar V E, Jangwoo Park, and Yongyun Cho. 'Using data mining techniques for bike sharing demand prediction in metropolitan city.' Computer Communications, Vol.153, pp.353-366, March, 2020. Feel free to look at this if you want more information about the dataset.\n",
    "\n",
    "The data is stored in a CSV (comma separated variable) file and contains the following information \n",
    "\n",
    " - Date: year-month-day\n",
    " - Rented Bike Count: Count of bikes rented at each hour\n",
    " - Hour: Hour of the day\n",
    " - Temperature: Temperature in Celsius\n",
    " - Humidity: %\n",
    " - Windspeed: m/s\n",
    " - Visibility: 10m\n",
    " - Dew point temperature: Celsius\n",
    " - Solar radiation: MJ/m2\n",
    " - Rainfall: mm\n",
    " - Snowfall: cm\n",
    " - Seasons: Winter, Spring, Summer, Autumn\n",
    " - Holiday: Holiday/No holiday\n",
    " - Functional Day: NoFunc(Non Functional Hours), Fun(Functional hours)\n",
    "\n",
    "**Load the data set from the csv file into a DataFrame, and summarise it with at least two appropriate pandas functions.**\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b7803-61e3-4181-b20b-714c0a1ec542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = ...\n",
    "\n",
    "# The following code is used by the autograder, don't change it\n",
    "step1_sol = data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67aa65-c02d-417f-8977-81edc91fea7c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff81dd-e750-4ee7-96c0-65d0f9152b1f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**STEP02:** To get a feeling for the data it is a good idea to do some form of simple visualisation. Display a set of histograms for the features as they are right now, prior to any cleaning steps.\n",
    "\n",
    "_Points:_ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686c030f-3cfe-4c56-86a3-1b6b0d52d4ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad69b11b-e5b9-4fe7-9609-6bc49b54fdfd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f972b-279b-4c48-b088-e342a264bce3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP03:** The \"Functioning Day\" feature records whether the bike rental was open for business on that day. For this assignment we are only interested in predicting demand on days when the business is open, so remove rows from the DataFrame where the business is closed. After doing this, delete the Functioning Day feature from the DataFrame and verify that this worked. **While you might solve this question in one way, be aware that there are multiple methods to achieve the same result in pandas, and these methods might be the subject of a question in the PracQuiz1 about this assignment.**\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13def6-6cd2-4350-90e7-d19d857b680c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = ...\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step3_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44529a2b-6879-4967-bce5-9a9e1fb7e1fc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba264cb-6af0-428c-9dd6-e9a3ee677f8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP04:** \n",
    "The goal is to predict bike rental demand using historical data. To achieve this, you will use regression techniques with \"Bike Rental Count\" as the target feature for this prediction, *but for this*, it is important that all other features in the data are numerical. Two of the features in the data, \"Holiday\" and \"Season\", need to be converted to numerical format. Write code to convert the \"Holiday\" feature to 0 or 1 from its current format. For the \"Season\" feature, add 4 new columns, labeled as \"Winter\", \"Spring\", \"Summer\", and \"Autumn\" then remove the \"Season\" column. Each of these columns should store a 0 or 1, depending on the corresponding season in each row. **While you might solve this question in one way, be aware that there are multiple methods to achieve the same result in pandas, and these methods might be the subject of a question in the PracQuiz1 about this assignment.**\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81b7dd-84ba-4eea-ba70-b676bd9fa185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "data = ...\n",
    "\n",
    "# The following code is used by the autograder, don't change it\n",
    "step4_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49402cfb-2331-4524-81ad-a750e8690edf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621bb16-d096-49b7-a73b-c4ecbd253f6e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP05**: It is known that bike rentals depend strongly on whether it's a weekday or a weekend. Replace the Date feature with a Weekday feature that stores 0 or 1 depending on whether the date represents a weekend or weekday. **While you might solve this question in one way, be aware that there are multiple methods to achieve the same result in pandas, and these methods might be the subject of a question in the PracQuiz1 about this assignment.**\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69895a7f-3694-40a1-acf0-36a48f4a5baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data ...\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step5_data\n",
    "step5_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354885a-254d-4790-a1db-0ce9bf7fccdf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3b55b-53b8-4293-a6a0-42c4b478ed0f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP06** Convert all the remaining data to numerical format, with any non-numerical entries set to NaN.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f87537-10a4-4216-9cc9-862acfe5e5f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = ...\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that DataFrame from this step is the one assigned to step6_data\n",
    "step6_data = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55748aaa-00d5-4475-96ae-b8b48188dcbf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step06\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17368290-e030-4f1f-9e25-a71386952da8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP07** Examin the data and identify problematic entries. Set any problematic values in the numerical data to `np.nan` and check that this has worked. Once this is done, specify a **sklearn *pipeline* that will perform imputation** to replace problematic entries (nan values) with an appropriate **median** value ***and* any other pre-processing** that you think should be used. Just specify the pipeline - do ***not*** run it now.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc32f339-8d1e-4351-8b7d-62a5f1a4b11f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data ...\n",
    "\n",
    "# keep the variable name pipeline_step7 as you will use it in STEP09\n",
    "pipeline_step7 = ...\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains the solutions from this step is the one assigned to step7_data as follows\n",
    "step7_data = [data.copy(),pipeline_step7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660fe6c-fa1a-4ceb-bba3-d8b8a40d79a1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c1079-68f5-4881-86b2-f45dd92ca557",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP08:** Generate a pre-processed version of the entire dataset by applying the pipeline defined in STEP07. Then, calculate the correlation of each feature with the target using either the pandas function corr() or numpy corrcoef() and find the 3 attributes that are the most correlated with bike rentals. \n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933518f8-fd5d-4df4-9f69-74b46a15aa51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_data = ...\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# top_3 should be an array of 3 strings ['attribute name', 'attribute name','attribute name']\n",
    "top_3 = ...\n",
    "print(top_3)\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains a list of the names of the top 3 attributes is assigned to step3_data\n",
    "step8_data = top_3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf23bf-5c11-4cb9-acc2-670fd7c7ef04",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step08\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5930044f-1a46-4cdd-bc31-11cf9ba86c40",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP09:** Divide the data into training and test sets using where 20% of the data is kept for testing. Create a pipeline that includes the linear regression model in addition to the pipeline defined in STEP07. Fit the pipeline to the training set and calculate the `rmse` of the fit to evaluate its performance. As a comparison, compute the `rmse` that would be obtained by predicting the mean value of bike rentals for all training examples.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab7b511-5127-44fc-95e0-2adf4e3ac2fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you can use make_pipline to create a new pipiline by adding a model at the end of pipeline_step7 or you can simply create a new pipeline. \n",
    "# whatever you end up doing, make sure it is called pipeline_step9\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "pipeline_step9 = ...\n",
    "\n",
    "y_train = ...\n",
    "X_train = ...\n",
    "\n",
    "y_test = ...\n",
    "X_test = ...\n",
    "\n",
    "\n",
    "# calculate the RMSE of the fit to the training data\n",
    "rmse_train = ...\n",
    "# calculate the RMSE of the baseline model (by predicting the mean value of bike rentals for all training examples)\n",
    "rmse_baseline = ...\n",
    "\n",
    "print(\"RMSE for training data:\", rmse_train)\n",
    "print(\"RMSE for baseline (predicting mean):\", rmse_baseline)\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step9_data = [rmse_train,rmse_baseline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f419c24-d2b5-4c36-90fa-ad9f64b53a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The following code will show a visualisation of the fit for your linear regression.\n",
    "# I will use your pipeline_step9 to predict on 200 points from the training data\n",
    "subset_size = 200\n",
    "y_train_pred = pipeline_step9.predict(X_train[:subset_size])\n",
    "\n",
    "# Then I create a scatterplot of predicted vs actual values using your variables from the cell above\n",
    "ax = sns.scatterplot(x=y_train[:subset_size], y=y_train_pred)\n",
    "# A perfect solution would look like the red line\n",
    "sns.lineplot(x=y_train[:subset_size], y=y_train[:subset_size], color='red')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88bd3ed-0f95-49a1-af42-2140c2f5c71b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step09\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b942b-85cd-41ff-9c2f-91f4e0b4b4af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP10:**  Fit a Kernel Ridge regression model (imported from sklearn.kernel_ridge) to the X_train data from STEP09. Build a new pipeline that includes the Kernel Ridge regression model in addition to the pipeline defined in STEP07, and fit it to the training data using default settings. Generate a scatter plot of the predicted values against the actual values for the training data, and calculate the RMSE of the fit to the training data.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc5180a-f6b0-466d-9ce7-b81a9b64f465",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# you can use make_pipline to create a new pipiline by adding a model at the end of pipeline_step7 or you can simply create a new pipeline. \n",
    "# whatever you end up doing, make sure it is called pipeline_step10\n",
    "pipeline_step10 = ...\n",
    "\n",
    "\n",
    "# make predictions on the training data\n",
    "y_pred_train_KR = ...\n",
    "\n",
    "# calculate rmse for training data\n",
    "rmse_train_KR = ...\n",
    "print('Kernel Ridge model RMSE on training data:', rmse_train_KR)\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step10_data = [rmse_train_KR,pipeline_step10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31dd3ae-dea6-4555-8d49-eb2dba3b82ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I will use your pipeline_step10 to predict on 200 points from the training data\n",
    "subset_size = 200\n",
    "y_train_pred = pipeline_step10.predict(X_train[:subset_size])\n",
    "\n",
    "# Then create a scatterplot of predicted vs actual values using your variables from the cell above\n",
    "ax = sns.scatterplot(x=y_train[:subset_size], y=y_train_pred)\n",
    "sns.lineplot(x=y_train[:subset_size], y=y_train[:subset_size], color='red')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8bdab-ad38-418a-84df-d5ebd183c178",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f98a96c-61a0-40b2-bb88-6eb1b431546c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP11:** Fit a Support Vector Regression (from sklearn.svm import SVR). As you did for STEP10, create a new pipeline using the pipelinr from STEP07 and this model and fit it to your training data, using the default settings. Again, calculate the RMSE of the fit to the training data.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3fcdb-18a2-42f2-ab54-b691a811a7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# you can use make_pipline to create a new pipiline by adding a model at the end of pipeline_step7 or you can simply create a new pipeline. \n",
    "# whatever you end up doing, make sure it is called pipeline_step11\n",
    "pipeline_step11 = ...\n",
    "\n",
    "# make predictions on the training data\n",
    "y_pred_train_SVR = ...\n",
    "\n",
    "# calculate rmse for training data\n",
    "rmse_train_SVR = ...\n",
    "print('Support Vector Regression model RMSE on training data:', rmse_train_SVR)\n",
    "\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step11_data = [rmse_train_SVR,pipeline_step11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a019c7c4-2e67-4111-9d1c-a136ae45d0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use your pipeline_step10 to predict on 200 points from the training data\n",
    "subset_size = 200\n",
    "y_train_pred = pipeline_step11.predict(X_train[:subset_size])\n",
    "\n",
    "# Then create a scatterplot of predicted vs actual values using your variables from the cell above\n",
    "ax = sns.scatterplot(x=y_train[:subset_size], y=y_train_pred)\n",
    "sns.lineplot(x=y_train[:subset_size], y=y_train[:subset_size], color='red')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4005b2-6645-4541-ab27-2b5cb27f531e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1323f6-6969-47ef-a614-419b4722cabb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP12:** Perform a 10 fold cross validation for each of the three model (LinearRegression,KernelRidge,SVR). This splits the training set (that we've used above) into 10 equal size subsets, and uses each in turn as the validation set while training a model with the other 9. You should therefore have 10 rmse values for each cross validation run. Find the mean and standard deviation of the rmse values obtained for each model for the validation splits.\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57801463-c15f-493c-bb4b-32670168b0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# you might need some or all of the following imports\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "#Linear Regression CV mean and std RMSE from the 10 folds:\n",
    "rmse_LR_mean = ...\n",
    "rmse_LR_std  = ...\n",
    "print('Linear Regression CV Scores:') \n",
    "print(f'Mean: {rmse_LR_mean:.2f}, Std: {rmse_LR_std:.2f}\\n')\n",
    "\n",
    "\n",
    "#Linear Regression CV mean and std:\n",
    "rmse_KR_mean = ...\n",
    "rmse_KR_std  = ...\n",
    "print('Kernel Ridge Regression CV Scores:') \n",
    "print(f'Mean: {rmse_KR_mean:.2f}, Std: {rmse_KR_std:.2f}\\n')\n",
    "\n",
    "\n",
    "#Linear Regression CV mean and std:\n",
    "rmse_SVR_mean = ...\n",
    "rmse_SVR_std  = ...\n",
    "print('Support Vector Regression CV Scores:') \n",
    "print(f'Mean: {rmse_SVR_mean:.2f}, Std: {rmse_SVR_std:.2f}\\n')\n",
    "\n",
    "\n",
    "# The following code is used by the autograder\n",
    "step12_data = [rmse_LR_mean,rmse_KR_mean,rmse_SVR_mean]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac329bf-9f88-450b-98f2-f0982e38777e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83026fa4-6a39-493e-adf8-db14904cc024",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "**STEP13:** Both the Kernel Ridge Regression and Support Vector Regression have hyperparameters that can be adjusted to suit the problem. Use grid search to systematically compare the generalisation performance (rmse) obtained with different hyperparameter settings (still with 10-fold CV). Use the sklearn function GridSearchCV to do this.\n",
    "\n",
    "For KernelRidge, vary the hyperparameter alpha. (note, if you are using KernelRidge as the last step in a pipeline, alpha is refered to as kernelridge__alpha) \n",
    "\n",
    "For SVR, vary the hyperparameter C. (note, if you are using SVR as the last step in a pipeline, C is refered to as SVR__C)\n",
    "\n",
    "Find the hyperparameter setting for each medel.\n",
    "\n",
    "Finally, train and apply both models, with the best hyperparameter settings, to the test set and report the performance as rmse.\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc456cd-be46-4a94-8d79-8c015560279e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Define the GridSearchCV objects for each model\n",
    "kr_cv = ...\n",
    "svr_cv = ...\n",
    "\n",
    "# Fit the GridSearchCV objects to the training data\n",
    "...\n",
    "...\n",
    "\n",
    "# Print the best hyperparameter setting for each model\n",
    "print(\"Best hyperparameter setting for Kernel Ridge Regression:\", kr_cv.best_params_)\n",
    "print(\"Best hyperparameter setting for Support Vector Regression:\", svr_cv.best_params_)\n",
    "\n",
    "# Create pipeline using the best hyperparameter\n",
    "pipeline_best_kr = ...\n",
    "pipeline_best_svr = ...\n",
    "\n",
    "# Train and apply the chosen model to the test set\n",
    "...\n",
    "kr_predictions = ...\n",
    "kr_rmse = ...\n",
    "\n",
    "...\n",
    "svr_predictions = ...\n",
    "svr_rmse = ...\n",
    "\n",
    "print(\"Kernel Ridge Regression RMSE on test set:\", kr_rmse)\n",
    "print(\"Support Vector Regression RMSE on test set:\", svr_rmse)\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains that data from this step is the one assigned to step3_data\n",
    "step13_data = [kr_rmse , svr_rmse, kr_cv , svr_cv,pipeline_best_kr,pipeline_best_svr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750bbb82-c4bf-491e-94db-24fdb26c405d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use your pipeline_best_svr to predict on 200 points from the training data\n",
    "subset_size = 200\n",
    "y_train_pred = pipeline_best_svr.predict(X_train[:subset_size])\n",
    "\n",
    "# Then create a scatterplot of predicted vs actual values using your variables from the cell above\n",
    "ax = sns.scatterplot(x=y_train[:subset_size], y=y_train_pred)\n",
    "sns.lineplot(x=y_train[:subset_size], y=y_train[:subset_size], color='red')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b7d78-6f91-4e88-8110-f46389bd9b83",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step13\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "otter": {
   "OK_FORMAT": false,
   "tests": {
    "step01": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step01\"\npoints = 2\n\n@test_case(points=2, hidden=False, \n    failure_message=\"data shape is incorrect\")\ndef test_q1(step1_sol):\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertEqual(step1_sol,(8760, 14),\"error\")\n",
    "step02": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step02\"\npoints = 0\n\n@test_case(points=0, hidden=False)\ndef test_q2():\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertEqual(1,1)\n",
    "step03": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step03\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"Did you make the required changes by removing the rows and the column based on the question specification?\")\ndef test_q3(step3_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertEqual(step3_data.loc[step3_data['Rented Bike Count'] == 0].shape[0],0)\n    TC.assertNotIn('Functioning Day', step3_data.columns)\n",
    "step04": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step04\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"Did you make the required changes by changing the rows and removing the column based on the question specification?\")\ndef test_q4(step4_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    D = step4_data\n    TC.assertIn('Holiday', D.columns)\n    TC.assertTrue((D['Holiday'] == 0).any())\n    TC.assertTrue((D['Holiday'] == 1).any())\n\n    # check that the 'Season' feature has been converted to 4 new columns\n    TC.assertIn('Winter', D.columns)\n    TC.assertIn('Spring', D.columns)\n    TC.assertIn('Summer', D.columns)\n    TC.assertIn('Autumn', D.columns)\n    TC.assertNotIn('Seasons', D.columns)\n",
    "step05": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step05\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"Did you make the required changes on the question specification?\")\ndef test_q5(step5_data):\n    import pandas as pd\n    import unittest as UT\n    TC = UT.TestCase()\n    D = step5_data\n    TC.assertTrue(pd.api.types.is_numeric_dtype(D['Weekday']))\n    TC.assertIn('Weekday', D.columns)\n    TC.assertNotIn('Date', D.columns)\n",
    "step06": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step06\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"Did you make the required changes based on the question specification?\")\ndef test_q6(step6_data):\n    import pandas as pd\n    import unittest as UT\n    TC = UT.TestCase()\n    D = step6_data\n    for col in D.columns:\n        TC.assertTrue(pd.api.types.is_numeric_dtype(D[col]))\n",
    "step07": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step07\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"Your pipeline is not correct or there is still problematic entries in your data\")\ndef test_q7(step7_data):\n    from sklearn.pipeline import Pipeline\n    import unittest as UT\n    TC = UT.TestCase()\n    D = step7_data[0]\n    pipeline_step7 = step7_data[1]\n    TC.assertIsInstance(pipeline_step7, Pipeline)\n    #TC.assertIsInstance(pipeline_step7.named_steps['imputer'], SimpleImputer)\n    TC.assertTrue(D['Humidity (%)'].isna().sum() > 0)\n",
    "step08": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step08\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"Your top3 list is not correct\")\ndef test_q8(step8_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertListEqual(sorted(list(step8_data)),sorted(['Hour', 'Temperature (C)', 'Winter']))\n",
    "step09": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step09\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"rmse for the baseline is better than your trained model\")\ndef test_q9(step9_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    rmse_train    = step9_data[0]\n    rmse_baseline = step9_data[1]\n    TC.assertTrue(rmse_train < rmse_baseline)\n    #TC.assertAlmostEqual(len(X_test)/(len(X_train) + len(X_test)),0.2)\n",
    "step10": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step10\"\npoints = 3\n\n@test_case(points=3, hidden=False, \n    failure_message=\"the RMSE of your model is too high or your pipeline is not correct\")\ndef test_q10(step10_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    rms = step10_data[0]\n    pipeline = step10_data[1]\n    TC.assertTrue(rms < 1000)\n    TC.assertTrue('KernelRidge' in str(pipeline.named_steps))\n",
    "step11": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step11\"\npoints = 4\n\n@test_case(points=4, hidden=False, \n    failure_message=\"the RMSE of your model is too high or your pipeline is not correct\")\ndef test_q11(step11_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    rms = step11_data[0]\n    pipeline = step11_data[1]\n    TC.assertTrue(rms < 800)\n    TC.assertTrue('SVR' in str(pipeline.named_steps))\n",
    "step12": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step12\"\npoints = 5\n\n@test_case(points=5, hidden=False, \n    failure_message=\"one or more RMSE of your models are too high or your pipeline is not correct\")\ndef test_q12(step12_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertTrue(step12_data[0] < 1000)\n    TC.assertTrue(step12_data[1] < 1000)\n    TC.assertTrue(step12_data[2] < 1000)\n",
    "step13": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"step13\"\npoints = 5\n\n@test_case(points=5, hidden=False, \n    failure_message=\"one or more RMSE of your models are too high or your pipeline is not correct\")\ndef test_q13(step13_data):\n    import unittest as UT\n    TC = UT.TestCase()\n    TC.assertTrue(step13_data[0] < 900)\n    TC.assertTrue(step13_data[1] < 300)\n    # Check grid search setup and results for KernelRidge\n    TC.assertIn('kernelridge__alpha', step13_data[2].param_grid)\n    TC.assertTrue(hasattr(step13_data[2], 'best_params_'))\n    TC.assertIn('kernelridge__alpha', step13_data[2].best_params_)\n    \n    # Check grid search setup and results for SVR\n    TC.assertIn('svr__C', step13_data[3].param_grid)\n    TC.assertTrue(hasattr(step13_data[3], 'best_params_'))\n    TC.assertIn('svr__C', step13_data[3].best_params_)\n\n"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
